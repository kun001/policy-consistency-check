# 文档解析与向量入库操作说明

本说明文档介绍后端的两条流水线：
- 文档解析与持久化流水线（落盘至 storage/ 并写入 SQLite）
- 向量化与 Weaviate 入库流水线（批量索引、失败重试、删除回滚）

配合《数据存储方案.md》，该说明帮助你快速完成文档从上传解析到入库向量的端到端操作与排查。

---

## 环境准备

- Python 3.10+
- 依赖包（至少）：
  - `fastapi`, `uvicorn`, `requests`
  - `weaviate-client`（用于连接 Weaviate）
- 安装示例：

```bash
pip install fastapi uvicorn requests weaviate-client
```

- 环境变量（可选）：
  - `STORAGE_ROOT`：覆盖存储根目录（默认 `py-backend/storage`）
  - `DB_FILE`：覆盖数据库文件路径（默认 `<STORAGE_ROOT>/db.sqlite3`）

- Weaviate 连接默认参数（见 `src/weaviate/weaviateEngine.py`）：
  - `DEFAULT_WEAVIATE_HTTP_HOST = "115.190.118.177"`
  - `DEFAULT_WEAVIATE_HTTP_PORT = 8080`
  - `DEFAULT_WEAVIATE_API_KEY = "key_kunkun"`
  - 如需自定义，在调用管线时传入 `client_params` 覆盖（示例见下文）。

---

## 启动后端

- `app.py` 在启动事件中自动初始化存储目录与 SQLite 表：

```bash
python app.py
# 默认监听 10010 端口
```

---

## 解析持久化流水线（Persist Pipeline）

入口函数：`src/storage/pipeline.py` 中的 `persist_parsed_document(...)`

作用：
- 将上传+解析产物落盘到 `storage/docs/<collection_id>/<doc_id>/raw/` 与 `parsed/`
- 写入 `documents` 与 `chunks` 表，维护文档状态与分块信息

写入的文件：
- `raw/<filename>`（原始上传文件）
- `parsed/content.txt`（正文）
- `parsed/toc.json`（层级目录树）
- `parsed/segments.json`（分段结构）
- `parsed/keywords.json`（关键词）

数据库更新：
- `documents.status`：从 `processing` 更新为 `succeeded`
- `documents.parsing_payload`：写入 `{ "chunk_count": <int> }`
- `chunks`：逐条插入，含 `title`、`content`、`section_path`、`chunk_index`

代码调用示例：

```python
from src.storage import init_storage_and_db, persist_parsed_document
from src.doc_structure_recognition import build_segments_struct
from src.utils import build_toc

init_storage_and_db()

file_content = "第一章 总则\n第一条 内容...\n第二条 内容...\n"
file_name = "policy.md"

file_struct = build_segments_struct(file_content=file_content, file_name=file_name)
segments = file_struct.get("segments", [])

# 构造 TOC（可选）
toc_tree, _ = build_toc(segments)

result = persist_parsed_document(
    temp_file_path="/tmp/upload.md",     # 上传临时文件路径
    filename=file_name,
    original_mime="text/markdown",
    file_content=file_content,
    segments=segments,
    toc=toc_tree,
    keywords=["政策", "规则"],
    collection_name="policy_documents",
)
print(result)
# {'collection_id': '...', 'doc_id': '...', 'paths': {...}, 'chunk_count': N}
```

HTTP API 接入：`POST /api/extract-segments`
- 表单参数：
  - `file`：上传文件
  - `persist`：`true/false`（是否落盘并写入数据库；默认 false）
  - `collection_name`：可选，集合名，默认 `policy_documents`

示例：

```bash
curl -X POST "http://localhost:10010/api/extract-segments" \
  -F "file=@./测试文件/政策文件示例.md" \
  -F "persist=true" \
  -F "collection_name=policy_documents"
```

---

## 向量化与 Weaviate 入库流水线（Embedding Pipeline）

入口函数：`src/storage/embedding_pipeline.py`
- `index_document_chunks(doc_id, *, collection_name, siliconflow_api_token, weaviate_api_key=None, client_params=None, batch_size=50, max_retries=2)`
  - 从 SQLite 读取指定 `doc_id` 的所有 `chunks`
  - 过滤空内容分块，批量调用 SiliconFlow 生成 embeddings
  - 批量写入 Weaviate（`_upsert_with_vectors`），失败重试
  - 回写 `chunks.weaviate_id` 与 `embedding_status`（成功为 `embedded`，失败为 `failed`）
  - 更新 `documents.status`：全部成功→`succeeded`，部分成功→`processing`，全失败→`failed`
  - 返回统计：`{"attempted":N, "uploaded":M, "failed":K}`

- `rollback_document_vectors(doc_id, *, collection_name, siliconflow_api_token, weaviate_api_key=None, client_params=None)`
  - 删除指定文档在 Weaviate 的所有对象
  - 本地回滚：将所有 chunks 状态置为 `pending`，清空 `weaviate_id/last_error`；文档状态改为 `uploaded`
  - 返回统计：`{"deleted_remote":X, "rolled_back":Y}`

调用示例：

```python
from src.storage import index_document_chunks, rollback_document_vectors

stats = index_document_chunks(
    doc_id="<doc_id>",
    collection_name="policy_documents",
    siliconflow_api_token="<your_siliconflow_token>",
    weaviate_api_key="key_kunkun",
    client_params={
        "http_host": "127.0.0.1",
        "http_port": 8080,
        "http_secure": False,
        "grpc_host": "127.0.0.1",
        "grpc_port": 50051,
        "grpc_secure": False,
        # 可覆盖 api_key: "..."
    },
    batch_size=50,
    max_retries=2,
)
print(stats)

rb = rollback_document_vectors(
    doc_id="<doc_id>",
    collection_name="policy_documents",
    siliconflow_api_token="<your_siliconflow_token>",
    weaviate_api_key="key_kunkun",
)
print(rb)
```

Weaviate 对象 UUID 映射：
- 若 `chunks.id` 为合法 UUID，将直接用作对象 ID
- 否则使用稳定映射：`uuid5(NAMESPACE_DNS, f"{collection_name}:{chunk_id}")`

写入属性映射：
- `content`：分块正文
- `title`：分块标题（无标题则使用 `chunk_id`）
- `metadata_json`：包含 `collection_id/doc_id/chunk_id/chunk_index/section_path`
- `source_id`：`chunk_id`

---

## 测试用例

- 解析管线测试：`py-backend/tests/test_ingest_pipeline.py`

```bash
python tests/test_ingest_pipeline.py
```

- 向量管线测试：`py-backend/tests/test_embedding_pipeline.py`
  - 依赖 Weaviate 客户端与可用的 Weaviate 服务。

```bash
python tests/test_embedding_pipeline.py
```

测试脚本注：运行时会将 `src` 加入 `sys.path`，以便直接导入 `storage` 包。

---

## 常见问题与排查

- `ModuleNotFoundError: No module named 'weaviate.classes'`
  - 未安装客户端：`pip install weaviate-client`

- 连接 Weaviate 失败
  - 检查 `DEFAULT_WEAVIATE_*` 或在调用时传入 `client_params` 覆盖连接参数
  - 确认网络可达、API KEY 有效、防火墙设置、服务状态

- `SiliconFlow` 嵌入返回为空或尺寸不匹配
  - 检查 `siliconflow_api_token` 与 `model` 设置，留意速率限制与超时

- 数据落盘路径不符合预期
  - 可设置 `STORAGE_ROOT` 指定根目录；落盘相对路径遵循《数据存储方案.md》

- 文档状态未更新
  - 解析管线：成功后应为 `succeeded`
  - 向量管线：全部成功→`succeeded`，部分成功→`processing`，全失败→`failed`

---

## 最佳实践与建议

- 上传解析与持久化建议分阶段执行：先 `persist` 落盘与入库，再调用 `index_document_chunks` 完成向量化入库。
- 可在 `chunks` 表增加本地向量缓存字段（`BLOB/TEXT`），用于断点续传或快速比对（后续可选）。
- 若并发增大，建议迁移到 Postgres/MySQL 并将 `TEXT` JSON 字段替换为 `JSONB/JSON`（详见《数据存储方案.md》）。

---

## 参考

- 存储方案与 Schema：`py-backend/docs/数据存储方案.md`
- 解析结构生成：`src/doc_structure_recognition.py`（`build_segments_struct`、`format_segments_output`）
- 存储模块：`src/storage/`（`repositories.py`, `pipeline.py`, `embedding_pipeline.py`）
- Weaviate 接入：`src/weaviate/weaviateEngine.py`、`api/weaivateApi.py`